mutate(month = month(date)) %>%
group_by(year, month) %>%
summarise(total = sum(prcp, na.rm = TRUE)) %>%
ggplot(
aes(
x = month,
y = total
)
) +
geom_col() +
facet_wrap(~year)
install.packages("rnoaa")
library(rnoaa)
?rnoaa
รง
ncdc_*
browseVignettes("rnoaa")
library(rnoaa)
library(rnoaa)
browseVignettes(rnoaa)
library(rnoaa)
browseVignettes(rnoaa)
browseVignettes("rnoaa"")
browseVignettes("rnoaa")
vignette
vignette(rnoaa)
storm_
rnoaa
library(rnoaa)
devtools::install_github("ropensci/rnoaa")
install.packages("devtools")
devtools::install_github("ropensci/rnoaa")
library("rnoaa")
ncdc(datasetid = 'GHCND', stationid = 'GHCND:USW00014895', startdate = '2013-10-01',
enddate = '2013-12-01')
?rnoaa
?rnoaa
detach("package:rnoaa", unload = TRUE)
library(rnoaa)
library(rnoaa)
ncdc()
?ncdc()
library(tidyverse)
install.packages("haven")
orr.stata <- read_dta(file = "./Downloads/orr_prm_1975_2018_v1.dta")
library(tidyverse)
library(haven)
orr.stata <- read_dta(file = "./Downloads/orr_prm_1975_2018_v1.dta")
View(orr.stata)
library(janitor)
clean_names()
oor <- oor.stata %>%
clean_names()
orr <- orr.stata %>%
clean_names()
View(orr)
colnames(orr)
View(orr)
orr.stata %>%
group_by(
citizenship_stable,
longitude,
latitude
) %>%
summarise(
sum = sum*refugees
)
orr.stata %>%
mutate(refugees = as.numeric(refugees))
orr.stata %>%
mutate(refugees = as.numeric(refugees)) %>%
group_by(
citizenship_stable,
longitude,
latitude
) %>%
summarise(
sum = sum(refugees, na.rm = T)
)
orr.stata %>%
mutate(refugees = as.numeric(refugees)) %>%
group_by(
citizenship_stable,
longitude,
latitude
) %>%
summarise(
totalRefugees = sum(refugees, na.rm = T)
)
#Load packages
library(sp)
library(rgdal)
library(reshape)
install.packages("reshape")
install.packages("maptools")
library(reshape)
library(ggplot2)
library(maptools)
library(rgeos) #create a range standardisation function
install.packages("rgeos")
library(rgeos) #create a range standardisation function
orr.stata %>%
mutate(refugees = as.numeric(refugees)) %>%
group_by(
citizenship_stable,
longitude,
latitude
) %>%
summarise(
totalRefugees = sum(refugees, na.rm = T)
)
orr.stata %>%
mutate(refugees = as.numeric(refugees)) %>%
group_by(
# citizenship_stable,
longitude,
latitude
) %>%
summarise(
totalRefugees = sum(refugees, na.rm = T)
)
orr.stata %>%
mutate(refugees = as.numeric(refugees)) %>%
group_by(
# citizenship_stable,
longitude,
latitude
) %>%
summarise(
totalRefugees = sum(refugees, na.rm = T)
) %>%
write_csv("Desktop/refugees.csv")
orr.stata %>%
mutate(refugees = as.numeric(refugees)) %>%
group_by(
citizenship_stable,
longitude,
latitude
) %>%
summarise(
totalRefugees = sum(refugees, na.rm = T)
)
orr.stata %>%
mutate(refugees = as.numeric(refugees)) %>%
group_by(
citizenship_stable,
longitude,
latitude
) %>%
summarise(
totalRefugees = sum(refugees, na.rm = T)
) %>%
pull(citizenship_stable)
orr.stata %>%
mutate(refugees = as.numeric(refugees)) %>%
group_by(
citizenship_stable,
longitude,
latitude
) %>%
summarise(
totalRefugees = sum(refugees, na.rm = T)
) %>%
pull(citizenship_stable) %>%  distinct()
orr.stata %>%
mutate(refugees = as.numeric(refugees)) %>%
group_by(
citizenship_stable,
longitude,
latitude
) %>%
summarise(
totalRefugees = sum(refugees, na.rm = T)
) %>%
pull(citizenship_stable) %>%unique()
library("jsonlite")
json_file <- 'https://datahub.io/JohnSnowLabs/country-and-continent-codes-list/datapackage.json'
json_data <- fromJSON(paste(readLines(json_file), collapse=""))
# get list of all resources:
print(json_data$resources$name)
# print all tabular data(if exists any)
for(i in 1:length(json_data$resources$datahub$type)){
if(json_data$resources$datahub$type[i]=='derived/csv'){
path_to_file = json_data$resources$path[i]
data <- read.csv(url(path_to_file))
print(data)
}
}
View(data)
# print all tabular data(if exists any)
for(i in 1:length(json_data$resources$datahub$type)){
if(json_data$resources$datahub$type[i]=='derived/csv'){
path_to_file = json_data$resources$path[i]
countryToContinent <- read.csv(url(path_to_file))
print(data)
}
}
orr %>%
mutate(refugees = as.numeric(refugees)) %>%
group_by(
citizenship_stable,
longitude,
latitude
) %>%
summarise(
totalRefugees = sum(refugees, na.rm = T)
)
orr.sum <- orr %>%
mutate(refugees = as.numeric(refugees)) %>%
group_by(
citizenship_stable,
longitude,
latitude
) %>%
summarise(
totalRefugees = sum(refugees, na.rm = T)
)
orr.sum
View(countryToContinent)
# print all tabular data(if exists any)
for(i in 1:length(json_data$resources$datahub$type)){
if(json_data$resources$datahub$type[i]=='derived/csv'){
path_to_file = json_data$resources$path[i]
countryToContinent <- read.csv(url(path_to_file)) %>%  clean_names()
print(data)
}
}
orr.sum %>%
rename(country_name = citizenship_stable) %>%
left_join(countryToContinent)
orr.sum %>%
rename(country_name = citizenship_stable)
?rename
orr.sum %>%
dplyr::rename(country_name = citizenship_stable)
orr.sum %>%
dplyr::rename(country_name = citizenship_stable) %>%
left_join(countryToContinent)
orr.sum %>%
dplyr::rename(country_name = citizenship_stable) %>%
left_join(countryToContinent) %>%
View()
countryToContinent %>%
write_csv("Desktop/countryToContinent.csv")
orr.sum <- orr %>%
mutate(refugees = as.numeric(refugees)) %>%
group_by(
citizenship_stable,
longitude,
latitude
) %>%
summarise(
totalRefugees = sum(refugees, na.rm = T)
) %>%
write_csv("Desktop/refugees.csv")
pull(countryToContinent$country_name)
?pull
dplyr:;pull(countryToContinent$country_name)
dplyr::pull(countryToContinent$country_name)
distinct(countryToContinent$country_name)
countryToContinent$country_name
unique(countryToContinent$country_name)
pull(countryToContinent, country_name)
orr.stata <- read_dta(file = "./Downloads/orr_prm_1975_2018_v1.dta")
orr <- orr.stata %>%
clean_names()
orr.sum <- orr %>%
mutate(refugees = as.numeric(refugees)) %>%
group_by(
citizenship_stable,
longitude,
latitude
) %>%
summarise(
totalRefugees = sum(refugees, na.rm = T)
) %>%
write_csv("Desktop/refugees.csv")
library("jsonlite")
json_file <- 'https://datahub.io/JohnSnowLabs/country-and-continent-codes-list/datapackage.json'
json_data <- fromJSON(paste(readLines(json_file), collapse=""))
# get list of all resources:
print(json_data$resources$name)
# print all tabular data(if exists any)
for(i in 1:length(json_data$resources$datahub$type)){
if(json_data$resources$datahub$type[i]=='derived/csv'){
path_to_file = json_data$resources$path[i]
countryToContinent <- read.csv(url(path_to_file)) %>%  clean_names()
print(data)
}
}
orr.sum %>%
dplyr::rename(country_name = citizenship_stable) %>%
left_join(countryToContinent) %>%
View()
View(orr)
orr.sum <- orr %>%
mutate(refugees = as.numeric(refugees)) %>%
group_by(
citizenship_stable,
longitude,
latitude
) %>%
summarise(
totalRefugees = sum(refugees, na.rm = T)
)
View(orr.sum)
orr.sum <- orr %>%
mutate(refugees = as.numeric(refugees)) %>%
group_by(
citizenship_stable,
fips
) %>%
summarise(
totalRefugees = sum(refugees, na.rm = T)
)
orr.sum <- orr %>%
mutate(refugees = as.numeric(refugees)) %>%
group_by(
# citizenship_stable,
fips
) %>%
summarise(
totalRefugees = sum(refugees, na.rm = T)
)
View(orr)
orr.sum <- orr %>%
filter(county_fips == ".")
orr.sum <- orr %>%
filter(county_fips != ".") %>%
mutate(refugees = as.numeric(refugees)) %>%
group_by(
# citizenship_stable,
fips
) %>%
summarise(
totalRefugees = sum(refugees, na.rm = T)
)
write_csv(orr.sum, "Desktop/refugees.csv")
require("httr")         # wrapper for curl package - may require installation
datalist = list()       # a list that will hold the results of each call
baseUrl <- "https://www.fema.gov/api/open/v1/FemaWebDisasterDeclarations?"
# Determine record count. Specifying only 1 column here to reduce amount of data returned.
#   Remember to add criteria/filter here (if you have any) to get an accurate count.
result <- GET(paste0(baseUrl,"$inlinecount=allpages&$top=1&$select=id"))
jsonData <- content(result)         # should automatically parse as JSON as that is mime type
recCount <- jsonData$metadata$count
# calculate the number of calls we will need to get all of our data (using the maximum of 1000)
top <- 1000
loopNum <- ceiling(recCount / top)
# send some logging info to the console so we know what is happening
print(paste0("START ",Sys.time(),", ", recCount, " records, ", top, " returned per call, ", loopNum," iterations needed."),quote=FALSE)
# Loop and call the API endpoint changing the record start each iteration. Each call will
# return results in a JSON format. The metadata has been suppressed as we no longer need it.
skip <- 0
for(i in seq(from=0, to=loopNum, by=1)){
# As above, if you have filters, specific fields, or are sorting, add that to the base URL
#   or make sure it gets concatenated here.
result <- GET(paste0(baseUrl,"$metadata=off&$top=",top,"&$skip=",i * top))
jsonData <- content(result)         # should automatically parse as JSON as that is mime type
# Here we are adding the resulting JSON return to a list that can be turned into a combined
#   dataframe later or saved. You may encounter memory limitations with very large datasets.
#   For those, inserting into a database or saving chunks of data may be desired.
datalist[[i+1]] <- jsonData
print(paste0("Iteration ", i, " done)"), quote=FALSE)
}
# binds many items in our list to one data frame
fullData <- dplyr::bind_rows(datalist)
# Save as one R object - probably more useful (and storage efficient) than CSV or JSON if doing
#   analysis within R.
saveRDS(fullData, file = "output.rds")
# open file just to verify that we got what we expect
my_data <- readRDS(file = "output.rds")
print(paste0("END ",Sys.time(), ", ", nrow(my_data), " records in file"))
View(fullData)
View(result)
View(my_data)
print(paste0("END ",Sys.time(), ", ", nrow(my_data), " records in file"))
View(fullData)
View(my_data)
my_data[[1]]
as.data.frame(my_data[[1]])
do.call(rbind.data.frame, my_data[[1]])
my_data[[1,]]
my_data[[,1]]
my_data[[1]]
install.packages("rds2csv")
df <- rds2csv("output.rds")
library(rds2csv)
df <- rds2csv("output.rds")
write.csv(my_data, "first.csv", row.names=FALSE)
library(tidyverse)
write.csv(my_data, "first.csv", row.names=FALSE)
library(todyverse)
library(tidyverse)
df <- read_csv("Desktop/fema-disaster-summaries.csv")
library(tigris)
options(tigris_use_cache = TRUE)`
options(tigris_use_cache = TRUE)
options(tigris_use_cache = TRUE)
library(tigris)
us_counties <- counties(cb=TRUE)
us_counties <- states(cb=TRUE)
ggplot(us_counties) +
geom_sf() +
theme_void()
ggplot(us_counties) +
geom_sf()
รงรง
dcvd
ggplot(us_counties) +
geom_sf()
manhattan_roads <- roads("NY", "New York")
ggplot(manhattan_roads) +
geom_sf() +
theme_void()
us_states <- states()
ggplot(us_states) +
geom_sf()
df %>%
filter(incidentType == "Flood") %>%
group_by(fyDeclared, fipsStateCode, fipsCountyCode) %>%
summarise(count = n())
df %>%
filter(incidentType == "Flood") %>%
group_by(fyDeclared, fipsStateCode) %>%
summarise(count = n()) %>%
ggplot(aes(
x = fyDeclared,
y = count,
group = fipsStateCode
)) +
geom_point()
df %>%
filter(incidentType == "Flood") %>%
group_by(fyDeclared, fipsStateCode) %>%
summarise(count = n()) %>%
ggplot(aes(
x = fyDeclared,
y = count,
group = fipsStateCode
)) +
geom_line()
df %>%
filter(incidentType == "Flood") %>%
group_by(fyDeclared) %>%
summarise(count = n()) %>%
ggplot(aes(
x = fyDeclared,
y = count,
group = fipsStateCode
)) +
geom_line()
df %>%
filter(incidentType == "Flood") %>%
group_by(fyDeclared) %>%
summarise(count = n()) %>%
ggplot(aes(
x = fyDeclared,
y = count
)) +
geom_line()
# make a df of the basemaps you want to include. You can get all available
# options via the built in `dw_basemaps` object
locals <-tibble::tribble(
~local,                             ~id,
"Local: Charlotte",       "north-carolina-counties",
"Local: Des Moines",                 "iowa-counties",
"Local: Tampa",              "florida-counties",
"Local: Denver",             "colorado-counties",
"Local: Twin Cities",            "minnesota-counties",
"Local: Northwest Arkansas",             "arkansas-counties",
"Local: DC", "district-of-columbia-counties",
"Local: Nashville",            "tennessee-counties",
"Local: Chicago",             "illinois-counties",
"Local: Austin/Dallas",                "texas-counties",
"Local: Columbus",                 "ohio-counties",
"Local: Atlanta",              "georgia-counties",
"Local: Philly",         "pennsylvania-counties"
)
clipr::write_clip(locals)
dw_retrieve_chart_metadata(pMsqV, api_key = "environment")
library(DatawRappr)
library(DatawRappr)
dw_retrieve_chart_metadata(pMsqV, api_key = "environment")
dw_retrieve_chart_metadata("pMsqV", api_key = "environment")
x <- dw_retrieve_chart_metadata("pMsqV", api_key = "environment")
x$content$metadata$visualize$basemap
setwd("~/Documents/GitHub/datawrapper-api-workflow/batch-datawrapper-generator")
locals <- read.csv("data/locals.csv")
x$content$metadata$visualize$basemapc
dw_basemaps
dw_basemaps
dw_basemaps()
dw_basemaps
Sys.getenv("DW_KEY")
dw_basemaps
dw_basemaps$id
dw_basemaps
x$content$metadata$visualize$basemap
x$content$metadata$visualize
x$content$metadata
x$path
x$content
x$content$externalData
c< -dw_retrieve_chart_metadata("iMgBR")
x <- dw_retrieve_chart_metadata("iMgBR")
x$content$metadata$visualize$basemap
x$content$metadata$visualize$basemap
x$content$metadata$data
x$content$metadata$data$`column-format`
x$content$metadata$visualize$basemap
x$content$metadata$publish
